{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RitikaVerma55/Student-Engagement-Analysis-Posture-Emotion-Tracking/blob/main/Posture_Emotion_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRofB1vsxINB"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python"
      ],
      "metadata": {
        "id": "HlyOPJ91xQqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe"
      ],
      "metadata": {
        "id": "rr7F2BvWxQnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "SJxvQtXnxQkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib"
      ],
      "metadata": {
        "id": "DZYe0xv2xYI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check if GPU is available\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    device = 'cpu'\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "id": "y3GB7_TdxQin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import mediapipe as mp"
      ],
      "metadata": {
        "id": "TwD2yR35xQgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mp_holistic = mp.solutions.holistic # Holistic model\n",
        "mp_drawing = mp.solutions.drawing_utils # drawing Utilities"
      ],
      "metadata": {
        "id": "YxKLeq3bxQdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mediapipe_detection(image, model):\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  image.flags.writeable = False\n",
        "  results = model.process(image)              # making predictions for joints\n",
        "  image.flags.writeable =True\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "  return image, results\n"
      ],
      "metadata": {
        "id": "M-m-LHCsxQa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_landmarks(image, results):\n",
        "    # Draw face landmarks with smaller circles and thinner lines\n",
        "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
        "                               landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1,\n",
        "                                                                             circle_radius=1))\n",
        "\n",
        "    # Draw pose landmarks with thinner lines\n",
        "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
        "                               landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=1))\n",
        "\n",
        "    # Draw hand landmarks with smaller circles and thinner lines\n",
        "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                               landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=1,\n",
        "                                                                             circle_radius=1))\n",
        "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                               landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=1,\n",
        "                                                                             circle_radius=1))\n"
      ],
      "metadata": {
        "id": "qnsCCZEOxQYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "VqGA0OgvxQVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keypoints(results):\n",
        "  pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
        "  face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
        "  lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
        "  rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
        "  return np.concatenate([pose, face, lh, rh])"
      ],
      "metadata": {
        "id": "mWW0rEKUxQTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imgaug"
      ],
      "metadata": {
        "id": "X0OqzEIBxQQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from imgaug import augmenters as iaa\n",
        "\n",
        "\n",
        "DATA_PATH = '/content/drive/MyDrive/DATA_PATH'\n",
        "actions = [ 'dozz', 'study', 'yawn']\n",
        "POSTURES_PATH = '/content/drive/MyDrive/posture_1'\n",
        "sequence_length = 50\n",
        "\n",
        "\n",
        "# Function to process a video file and save keypoints\n",
        "def process_video(file_path, action, sequence_count, augmentation):\n",
        "    cap = cv2.VideoCapture(file_path)\n",
        "    frame_count = 0\n",
        "\n",
        "    # Set mediapipe model\n",
        "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
        "        # Loop through video frames\n",
        "        while cap.isOpened() and frame_count < sequence_length:\n",
        "            # Read feed\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if augmentation:\n",
        "                # Data augmentation\n",
        "                augmented_frame = augmentation.augment_image(frame)\n",
        "                frame = augmented_frame\n",
        "\n",
        "\n",
        "            # Make detections\n",
        "            image, results = mediapipe_detection(frame, holistic)\n",
        "\n",
        "            # Extract keypoints\n",
        "            keypoints = extract_keypoints(results)\n",
        "\n",
        "\n",
        "            # Export keypoints\n",
        "            sequence_dir = os.path.join(DATA_PATH, action, f\"sequence_{sequence_count}\")\n",
        "            if not os.path.exists(sequence_dir):\n",
        "                os.makedirs(sequence_dir)\n",
        "\n",
        "            # Save keypoints for current frame\n",
        "            npy_path = os.path.join(sequence_dir, f\"frame_{frame_count}.npy\")\n",
        "            print(keypoints)\n",
        "            np.save(npy_path, keypoints)\n",
        "\n",
        "            # Increment frame counter\n",
        "            frame_count += 1\n",
        "\n",
        "            # Show frame with landmarks\n",
        "            cv2.waitKey(100)  # Adjust the delay as needed\n",
        "\n",
        "        # Release the video capture object\n",
        "        cap.release()\n",
        "\n",
        "# Loop through each action folder\n",
        "for action in actions:\n",
        "    action_folder = os.path.join(POSTURES_PATH, action)\n",
        "    sequence_count = 0\n",
        "\n",
        "    # Iterate over each file in the action folder\n",
        "    for file in os.listdir(action_folder):\n",
        "        file_path = os.path.join(action_folder, file)\n",
        "\n",
        "        # Ensure that the item is a file and ends with '.mp4' extension\n",
        "        if os.path.isfile(file_path) and file.endswith('.mp4'):\n",
        "            print(f\"Processing video file: {file}\")\n",
        "\n",
        "            # Process original video without augmentation\n",
        "            process_video(file_path, action, sequence_count, None)\n",
        "\n",
        "\n",
        "            # Process augmented video with augmentation\n",
        "            sequence_count += 1\n"
      ],
      "metadata": {
        "id": "a4tiLkiVxQOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "mj5QOKMSxQMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {label:num for num, label in enumerate(actions)}\n",
        "print(label_map)"
      ],
      "metadata": {
        "id": "GN5udVfoxQJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences, labels = [], []\n",
        "\n",
        "# Loop through each action\n",
        "for action in actions:\n",
        "    action_dir = os.path.join(DATA_PATH, action)\n",
        "\n",
        "    # Loop through each sequence directory\n",
        "    for sequence_dir in os.listdir(action_dir):\n",
        "        sequence_path = os.path.join(action_dir, sequence_dir)\n",
        "\n",
        "        if os.path.isdir(sequence_path):  # Check if it's a directory\n",
        "            window = []\n",
        "\n",
        "            # Loop through each frame in the sequence\n",
        "            for frame_num in range(sequence_length):\n",
        "                npy_filename = f\"frame_{frame_num}.npy\"\n",
        "                npy_path = os.path.join(sequence_path, npy_filename)\n",
        "\n",
        "                if os.path.exists(npy_path):  # Check if the file exists\n",
        "                    res = np.load(npy_path, allow_pickle=True)\n",
        "\n",
        "                    # Debugging: Print shape of keypoints extracted for each frame\n",
        "                    print(f\"Shape of keypoints for frame {frame_num}: {res.shape}\")\n",
        "\n",
        "                    window.append(res)\n",
        "                else:\n",
        "                    print(f\"File not found: {npy_path}\")\n",
        "\n",
        "            # Append window to sequences\n",
        "            sequences.append(window)\n",
        "            labels.append(label_map[action])\n",
        "\n",
        "# Debugging: Print shape of the constructed sequences array\n",
        "print(f\"Shape of sequences array: {np.array(sequences).shape}\")\n"
      ],
      "metadata": {
        "id": "fSSlyT5_xQG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(sequences).shape"
      ],
      "metadata": {
        "id": "1G9cSRRoxvln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(sequences)\n",
        "y = to_categorical(labels).astype(int)"
      ],
      "metadata": {
        "id": "AZdzakZxxviG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# First, split the dataset into train and temporary (val_test) sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_val shape:\", y_val.shape)\n"
      ],
      "metadata": {
        "id": "cr70Kqe0xvf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "F8BxCDJvxvdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import ConvLSTM2D, Dense, Flatten\n",
        "from tensorflow.keras.callbacks import TensorBoard\n"
      ],
      "metadata": {
        "id": "feZrE1ZgxvbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.expand_dims(X_train, axis=1)\n",
        "X_val = np.expand_dims(X_val, axis =1)"
      ],
      "metadata": {
        "id": "wV9zEUqUxvYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "lZVn_E6NxvVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val.shape"
      ],
      "metadata": {
        "id": "01YooaZ8xvTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "8RXhgddqxvRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_actions = len(actions)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=(1, 50, 1662, 1), return_sequences=True, padding='same'))\n",
        "model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3), activation='relu', return_sequences=False, padding='same'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(num_actions, activation='softmax'))\n",
        "\n"
      ],
      "metadata": {
        "id": "Lmhaz4pTxvNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "optimizer = Adam(learning_rate=0.000001)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "aYtp-2iOxvLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "3dTmoWxyxvI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=1, validation_data=(X_val, y_val), callbacks=[early_stopping])\n"
      ],
      "metadata": {
        "id": "dXcf4iMKyBiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the plot_learning_curve function\n",
        "def plot_learning_curve(history):\n",
        "    # Extract loss and validation loss from the history object\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    # Extract epochs from the history object\n",
        "    epochs = range(1, len(loss) + 1)\n",
        "\n",
        "    # Plot loss and validation loss\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Assuming `history` object is available from the model training\n",
        "plot_learning_curve(history)\n"
      ],
      "metadata": {
        "id": "6bgODA6NyBfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "Metv5BHoyBcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################################################\n",
        "#######################################################################################"
      ],
      "metadata": {
        "id": "FnsIf9Y3yBZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "data_path = '/content/drive/MyDrive/emotion_1_train/train'\n",
        "data_dir_list = os.listdir(data_path)\n",
        "\n",
        "num_channel = 1  # Since your images are grayscale\n",
        "num_epoch = 10\n",
        "\n",
        "img_data_list = []\n",
        "\n",
        "for dataset in data_dir_list:\n",
        "    img_list = os.listdir(os.path.join(data_path, dataset))\n",
        "    print('Resizing images to 48x48 for dataset - {}\\n'.format(dataset))\n",
        "    for img in img_list:\n",
        "        input_img = cv2.imread(os.path.join(data_path, dataset, img))\n",
        "        input_img_resize = cv2.resize(input_img, (48, 48))\n",
        "        img_data_list.append(input_img_resize)\n",
        "\n",
        "img_data = np.array(img_data_list)\n",
        "img_data = img_data.astype('float32') / 255.0  # Normalize the image data"
      ],
      "metadata": {
        "id": "zXkREJ9byBW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 3\n",
        "\n",
        "num_of_samples = img_data.shape[0]\n",
        "labels = np.ones((num_of_samples,),dtype='int64')\n",
        "\n",
        "labels[0:429]=0               #430\n",
        "labels[430:859]=1             #430\n",
        "labels[860:1289]=2            #430\n",
        "\n",
        "\n",
        "names = ['disgust','happy','neutral']\n",
        "\n",
        "def getLabel(id):\n",
        "    return ['disgust','happy','neutral'][id]"
      ],
      "metadata": {
        "id": "C46f5oLayBUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_1 = to_categorical(labels, num_classes)"
      ],
      "metadata": {
        "id": "sLhTHdtSyBSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_1.shape"
      ],
      "metadata": {
        "id": "3dp5dg26yBPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "Pc5UY9YWyBNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_1,y_1 = shuffle(img_data,Y_1)"
      ],
      "metadata": {
        "id": "SatisQ_TyBLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_1_train, X_1_val, y_1_train, y_1_val = train_test_split(x_1, y_1, test_size=0.2)"
      ],
      "metadata": {
        "id": "52htyXOlyBIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_1_train.shape"
      ],
      "metadata": {
        "id": "ACmF45x8yBGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_1_val.shape"
      ],
      "metadata": {
        "id": "nO6FKiItyBDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.metrics import categorical_accuracy\n",
        "from keras.models import model_from_json\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import *\n",
        "from keras.layers import BatchNormalization\n",
        "import os"
      ],
      "metadata": {
        "id": "ZLMUhMjgyA1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "#from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "input_shape = (48, 48, 3)\n",
        "\n",
        "emotion_model = Sequential()\n",
        "\n",
        "emotion_model.add(Conv2D(32, (3, 3), input_shape=input_shape, padding='same', activation='relu', kernel_regularizer=l2(0.001)))\n",
        "emotion_model.add(BatchNormalization())\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "emotion_model.add(Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.001)))\n",
        "emotion_model.add(BatchNormalization())\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "emotion_model.add(Conv2D(128, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.001)))\n",
        "emotion_model.add(BatchNormalization())\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "emotion_model.add(Flatten())\n",
        "\n",
        "emotion_model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "emotion_model.add(BatchNormalization())\n",
        "emotion_model.add(Dropout(0.3))\n",
        "\n",
        "emotion_model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Use Adam optimizer with a learning rate of 0.001\n",
        "optimizer = Adam(learning_rate=0.000001)\n",
        "\n",
        "emotion_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=optimizer)\n",
        "emotion_model.summary()"
      ],
      "metadata": {
        "id": "2N5GFjK_yYZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from keras import callbacks\n",
        "\n",
        "filename = 'emotion_model_train_new.csv'\n",
        "filepath = \"Best-weights-my_emotion_model-{epoch:03d}-{loss:.4f}-{acc:.4f}.hdf5.keras\"\n",
        "\n",
        "# Check if the CSV file exists\n",
        "if not os.path.exists(filename):\n",
        "    # If the file doesn't exist, create it\n",
        "    with open(filename, 'w') as f:\n",
        "        pass  # Create an empty file\n",
        "\n",
        "csv_log = callbacks.CSVLogger(filename, separator=',', append=False)\n",
        "checkpoint = callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [csv_log, checkpoint]\n",
        "callbacks_list = [csv_log]"
      ],
      "metadata": {
        "id": "wF4c7Qf-yYWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define the early stopping callback\n",
        "early_stopping_1 = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Add the early stopping callback to the list of callbacks\n",
        "callbacks_list.append(early_stopping_1)\n",
        "\n",
        "# Train the emotion_model with the added callbacks\n",
        "hist = emotion_model.fit(X_1_train, y_1_train, batch_size=3, epochs=100, verbose=1, validation_data=(X_1_val, y_1_val), callbacks=callbacks_list)"
      ],
      "metadata": {
        "id": "ZUDvu12SyYUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#rcParams['figure.figsize'] = 4, 4\n",
        "\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('emotion_model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-YWu9pE9yYR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################################################################################################################\n",
        "###################################################################################################################################################"
      ],
      "metadata": {
        "id": "cJYA6_q3yYPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_face(face_image):\n",
        "    resized_image = cv2.resize(face_image, (48, 48))\n",
        "    normalized_image = resized_image.astype('float32') / 255.0\n",
        "    preprocessed_image = np.expand_dims(normalized_image, axis=0)\n",
        "    return preprocessed_image\n",
        "\n",
        "def predict_emotion(emotion_model, preprocessed_image):\n",
        "    prediction = emotion_model.predict(preprocessed_image)\n",
        "    emotion_labels = ['disgust', 'Happy', 'Neutral']\n",
        "    predicted_emotion = emotion_labels[np.argmax(prediction)]\n",
        "    return predicted_emotion"
      ],
      "metadata": {
        "id": "ixwp5WvRyYM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to detect faces and predict emotions in a frame using facial keypoints\n",
        "def detect_and_predict_emotions_new(frame):\n",
        "\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = holistic.process(frame_rgb)\n",
        "\n",
        "    # Extract facial keypoints\n",
        "    face_landmarks = results.face_landmarks\n",
        "\n",
        "    # Check if facial keypoints are detected\n",
        "    if face_landmarks:\n",
        "        # Convert the facial keypoints to numpy array\n",
        "        landmarks_array = np.array([[res.x, res.y] for res in face_landmarks.landmark])\n",
        "\n",
        "        # Get the minimum and maximum x and y coordinates of the facial keypoints\n",
        "        min_x = int(np.min(landmarks_array[:, 0]) * frame.shape[1])\n",
        "        max_x = int(np.max(landmarks_array[:, 0]) * frame.shape[1])\n",
        "        min_y = int(np.min(landmarks_array[:, 1]) * frame.shape[0])\n",
        "        max_y = int(np.max(landmarks_array[:, 1]) * frame.shape[0])\n",
        "\n",
        "        # Extract the region of interest (face) from the frame\n",
        "        face = frame[min_y:max_y, min_x:max_x]\n",
        "\n",
        "        # Preprocess the face region for emotion detection\n",
        "        preprocessed_face = preprocess_face(face)\n",
        "\n",
        "        # Predict the emotion from the preprocessed face\n",
        "        predicted_emotion = predict_emotion(emotion_model, preprocessed_face)\n",
        "\n",
        "        # Write the predicted emotion on the frame\n",
        "        emotion_text = 'Emotion: ' + predicted_emotion\n",
        "        cv2.putText(frame, emotion_text, (min_x, min_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "        # Draw a rectangle around the detected face\n",
        "        cv2.rectangle(frame, (min_x, min_y), (max_x, max_y), (255, 0, 0), 2)\n",
        "\n",
        "    return frame\n",
        "\n"
      ],
      "metadata": {
        "id": "11KpN74hyYKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = [(245,117,16), (117,245,16), (16,117,245),(245,117,16),(117,245,16),(16,117,245)]\n",
        "def prob_viz(res, actions, input_frame, colors):\n",
        "    output_frame = input_frame.copy()\n",
        "    for num, prob in enumerate(res):\n",
        "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
        "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
        "\n",
        "    return output_frame"
      ],
      "metadata": {
        "id": "d71ja39myYIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow"
      ],
      "metadata": {
        "id": "3SRGuwCMyYGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install ffmpeg"
      ],
      "metadata": {
        "id": "RjQI8vrPyYDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "\n",
        "# Load the holistic model\n",
        "mp_holistic = mp.solutions.holistic\n",
        "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "\n",
        "# New detection variables\n",
        "sequence = []\n",
        "sentence = []\n",
        "predictions = []\n",
        "threshold = 0.02\n",
        "\n",
        "# Define actions\n",
        "actions = ['dozz', 'study', 'yawn']\n",
        "\n",
        "# Your existing code to initialize output directory\n",
        "output_frames_dir = '/content/drive/MyDrive/output_frames'\n",
        "os.makedirs(output_frames_dir, exist_ok=True)\n",
        "\n",
        "# Your existing code to capture video from the source\n",
        "cap = cv2.VideoCapture('/content/drive/MyDrive/man_with_headphones.mp4')\n",
        "\n",
        "frame_count = 0\n",
        "while cap.isOpened() and frame_count < 300:  # Limit to 300 frames for demo\n",
        "    # Read frame\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Make detections using the holistic model\n",
        "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB for Mediapipe\n",
        "    results = holistic.process(image)\n",
        "\n",
        "    # Get keypoints\n",
        "    keypoints = extract_keypoints(results)\n",
        "    sequence.append(keypoints)\n",
        "    sequence = sequence[-50:]\n",
        "\n",
        "    if len(sequence) == 50:\n",
        "        # Reshape sequence to match the input shape of the model\n",
        "        sequence_input = np.expand_dims(sequence, axis=0)  # Add batch dimension\n",
        "        sequence_input = np.expand_dims(sequence_input, axis=1)  # Add channel dimension\n",
        "        print(sequence_input.shape)\n",
        "\n",
        "        # Predict action\n",
        "        res = model.predict(sequence_input)[0]\n",
        "        predictions.append(np.argmax(res))\n",
        "\n",
        "        # Visualize prediction probabilities\n",
        "        image = prob_viz(res, actions, image, colors)\n",
        "\n",
        "        # Update sentence based on predictions and threshold\n",
        "        if np.unique(predictions[-10:])[0] == np.argmax(res) and res[np.argmax(res)] > threshold:\n",
        "            if len(sentence) > 0:\n",
        "                if actions[np.argmax(res)] != sentence[-1]:\n",
        "                    sentence.append(actions[np.argmax(res)])\n",
        "            else:\n",
        "                sentence.append(actions[np.argmax(res)])\n",
        "\n",
        "        if len(sentence) > 5:\n",
        "            sentence = sentence[-5:]\n",
        "\n",
        "    # Call emotion detection function\n",
        "    image = detect_and_predict_emotions_new(image)\n",
        "\n",
        "    # Draw landmarks\n",
        "    draw_landmarks(image, results)\n",
        "\n",
        "    # Call emotion detection function\n",
        "    #image = detect_and_predict_emotions_new(image)\n",
        "\n",
        "    # Draw predicted action text\n",
        "    cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
        "    cv2.putText(image, ' '.join(sentence), (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "    # Save annotated frame\n",
        "    cv2.imwrite(os.path.join(output_frames_dir, f'frame_{frame_count:04d}.jpg'), cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    # Show the frame\n",
        "    cv2_imshow(image)\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "\n",
        "# Create video from annotated frames\n",
        "output_video_path = '/content/drive/MyDrive/output_menHeadphn_video.mp4'\n",
        "os.system(f'ffmpeg -r 30 -i {output_frames_dir}/frame_%04d.jpg -vcodec libx264 -crf 25 -pix_fmt yuv420p {output_video_path}')\n"
      ],
      "metadata": {
        "id": "3J016FaGyYA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_styled_landmarks(image, results):\n",
        "    # Draw face connections\n",
        "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
        "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
        "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
        "                             )\n",
        "    # Draw pose connections\n",
        "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
        "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
        "                             )\n",
        "    # Draw left hand connections\n",
        "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
        "                             )\n",
        "    # Draw right hand connections\n",
        "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
        "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
        "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
        "                             )"
      ],
      "metadata": {
        "id": "xuAkLnuxABYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "\n",
        "# Load the holistic model\n",
        "mp_holistic = mp.solutions.holistic\n",
        "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "\n",
        "# New detection variables\n",
        "sequence = []\n",
        "sentence = []\n",
        "predictions = []\n",
        "threshold = 0.02\n",
        "\n",
        "# Define actions\n",
        "actions = ['dozz', 'study', 'yawn']\n",
        "\n",
        "# Your existing code to initialize output directory\n",
        "output_frames_dir = '/content/drive/MyDrive/output_frames'\n",
        "os.makedirs(output_frames_dir, exist_ok=True)\n",
        "\n",
        "# Your existing code to capture video from the source\n",
        "cap = cv2.VideoCapture('/content/drive/MyDrive/watermarked_preview (15).mp4')\n",
        "\n",
        "frame_count = 0\n",
        "while cap.isOpened() and frame_count < 300:  # Limit to 300 frames for demo\n",
        "    # Read frame\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Make detections using the holistic model\n",
        "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB for Mediapipe\n",
        "    results = holistic.process(image)\n",
        "\n",
        "    # Get keypoints\n",
        "    keypoints = extract_keypoints(results)\n",
        "    sequence.append(keypoints)\n",
        "    sequence = sequence[-50:]\n",
        "\n",
        "    if len(sequence) == 50:\n",
        "        # Reshape sequence to match the input shape of the model\n",
        "        sequence_input = np.expand_dims(sequence, axis=0)  # Add batch dimension\n",
        "        sequence_input = np.expand_dims(sequence_input, axis=1)  # Add channel dimension\n",
        "        print(sequence_input.shape)\n",
        "\n",
        "        # Predict action\n",
        "        res = model.predict(sequence_input)[0]\n",
        "        predictions.append(np.argmax(res))\n",
        "\n",
        "        # Visualize prediction probabilities\n",
        "        image = prob_viz(res, actions, image, colors)\n",
        "\n",
        "        # Update sentence based on predictions and threshold\n",
        "        if np.unique(predictions[-10:])[0] == np.argmax(res) and res[np.argmax(res)] > threshold:\n",
        "            if len(sentence) > 0:\n",
        "                if actions[np.argmax(res)] != sentence[-1]:\n",
        "                    sentence.append(actions[np.argmax(res)])\n",
        "            else:\n",
        "                sentence.append(actions[np.argmax(res)])\n",
        "\n",
        "        if len(sentence) > 5:\n",
        "            sentence = sentence[-5:]\n",
        "\n",
        "    # Call emotion detection function\n",
        "    image = detect_and_predict_emotions_new(image)\n",
        "\n",
        "    # Draw landmarks\n",
        "    draw_styled_landmarks(image, results)\n",
        "\n",
        "    # Call emotion detection function\n",
        "    #image = detect_and_predict_emotions_new(image)\n",
        "\n",
        "    # Draw predicted action text\n",
        "    cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
        "    cv2.putText(image, ' '.join(sentence), (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "    # Save annotated frame\n",
        "    cv2.imwrite(os.path.join(output_frames_dir, f'frame_{frame_count:04d}.jpg'), cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    # Show the frame\n",
        "    cv2_imshow(image)\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "\n",
        "# Create video from annotated frames\n",
        "output_video_path = '/content/drive/MyDrive/output_workingPerson_video.mp4'\n",
        "os.system(f'ffmpeg -r 30 -i {output_frames_dir}/frame_%04d.jpg -vcodec libx264 -crf 25 -pix_fmt yuv420p {output_video_path}')\n"
      ],
      "metadata": {
        "id": "pL0V5-2fyX-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "\n",
        "# Load the holistic model\n",
        "mp_holistic = mp.solutions.holistic\n",
        "holistic = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "\n",
        "# New detection variables\n",
        "sequence = []\n",
        "sentence = []\n",
        "predictions = []\n",
        "threshold = 0.02\n",
        "\n",
        "# Define actions\n",
        "actions = ['dozz', 'study', 'yawn']\n",
        "\n",
        "# Your existing code to initialize output directory\n",
        "output_frames_dir = '/content/drive/MyDrive/output_frames'\n",
        "os.makedirs(output_frames_dir, exist_ok=True)\n",
        "\n",
        "# Your existing code to capture video from the source\n",
        "cap = cv2.VideoCapture('/content/drive/MyDrive/little_girl.mp4')\n",
        "\n",
        "frame_count = 0\n",
        "while cap.isOpened() and frame_count < 300:  # Limit to 300 frames for demo\n",
        "    # Read frame\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Make detections using the holistic model\n",
        "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB for Mediapipe\n",
        "    results = holistic.process(image)\n",
        "\n",
        "    # Get keypoints\n",
        "    keypoints = extract_keypoints(results)\n",
        "    sequence.append(keypoints)\n",
        "    sequence = sequence[-50:]\n",
        "\n",
        "    if len(sequence) == 50:\n",
        "        # Reshape sequence to match the input shape of the model\n",
        "        sequence_input = np.expand_dims(sequence, axis=0)  # Add batch dimension\n",
        "        sequence_input = np.expand_dims(sequence_input, axis=1)  # Add channel dimension\n",
        "        print(sequence_input.shape)\n",
        "\n",
        "        # Predict action\n",
        "        res = model.predict(sequence_input)[0]\n",
        "        predictions.append(np.argmax(res))\n",
        "\n",
        "        # Visualize prediction probabilities\n",
        "        image = prob_viz(res, actions, image, colors)\n",
        "\n",
        "        # Update sentence based on predictions and threshold\n",
        "        if np.unique(predictions[-10:])[0] == np.argmax(res) and res[np.argmax(res)] > threshold:\n",
        "            if len(sentence) > 0:\n",
        "                if actions[np.argmax(res)] != sentence[-1]:\n",
        "                    sentence.append(actions[np.argmax(res)])\n",
        "            else:\n",
        "                sentence.append(actions[np.argmax(res)])\n",
        "\n",
        "        if len(sentence) > 5:\n",
        "            sentence = sentence[-5:]\n",
        "\n",
        "    # Call emotion detection function\n",
        "    image = detect_and_predict_emotions_new(image)\n",
        "\n",
        "    # Draw landmarks\n",
        "    draw_styled_landmarks(image, results)\n",
        "\n",
        "    # Call emotion detection function\n",
        "    #image = detect_and_predict_emotions_new(image)\n",
        "\n",
        "    # Draw predicted action text\n",
        "    cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)\n",
        "    cv2.putText(image, ' '.join(sentence), (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "    # Save annotated frame\n",
        "    cv2.imwrite(os.path.join(output_frames_dir, f'frame_{frame_count:04d}.jpg'), cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    # Show the frame\n",
        "    cv2_imshow(image)\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "\n",
        "# Create video from annotated frames\n",
        "output_video_path = '/content/drive/MyDrive/output_littleGirl_video.mp4'\n",
        "os.system(f'ffmpeg -r 30 -i {output_frames_dir}/frame_%04d.jpg -vcodec libx264 -crf 25 -pix_fmt yuv420p {output_video_path}')\n"
      ],
      "metadata": {
        "id": "BhgYkolJyX7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F2vD2c42yX5k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}